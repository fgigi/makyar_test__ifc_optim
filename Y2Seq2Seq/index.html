<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"> 
	<head>
		<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
		<meta content="all" name="robots" />
		<meta name="author" content="Ge Gao,Yu-Shen Liu,Pengpeng Lin,Meng Wang,Ming Gu,Jun-Hai Yong" />
		<meta name="Copyright" content="© BIM Research Group, School of Software, Tsinghua University," />
		<meta name="description" content="BIMTag: Concept-based automatic semantic annotation of online BIM product resources" />
		<meta name="keywords" content="BIMTag: Concept-based automatic semantic annotation of online BIM product resources" />
		<link rel="icon" href="../sub/favicon.ico" type="image/x-icon" />
		<link rel="shortcut icon" href="../sub/favicon.ico" type="image/x-icon" />
		<link rel="stylesheet" rev="stylesheet" href="../sub/index.css" type="text/css" media="all" />
		<title>Y^2Seq2Seq: Cross-Modal Representation Learning for 3D Shape and Text by
Joint Reconstruction and Prediction of View and Word Sequences</title>
	</head>
	<body>
		<div class="main">
			<div class="centerContent">
				<div class="banner">
				</div>
				<h1 class="paperTitle">Y^2Seq2Seq: Cross-Modal Representation Learning for 3D Shape and Text by
Joint Reconstruction and Prediction of View and Word Sequences</h1> 
				<p class="authors">
				<a>Zhizhong Han</a>,
				<a>Mingyang Shang</a>,
				<a>Xiyang Wang</a>,
				<a href="../">Yu-Shen Liu</a><sup><a class="star" href="#comm">*</a></sup>,
				<a>Matthias Zwicker</a>,
				</p> 
				<p class="contact">
					School of Software, Tsinghua University, Beijing, China <br/>
				</p>
				<hr/>
				<div class="itemContent" style="text-align:center"> 
				
				<a href="big/Y^2Seq2Seq.png"><img src="big/Y^2Seq2Seq.png" alt="" width="400px" ></a>
				
				<blockquote class="it"> 
						Figure 1: The framework of Y^2Seq2Seq.
				</blockquote>
				</div>
			</div>
			<div class="itemName">Abstract:</div>
			<div class="itemContent">
				<p>
					Jointly learning representations of 3D shapes and text is
crucial to support tasks such as cross-modal retrieval or
shape captioning. A recent method employs 3D voxels
to represent 3D shapes, but this limits the approach to
low resolutions due to the computational cost caused by
the cubic complexity of 3D voxels. Hence the method
suffers from a lack of detailed geometry. To resolve this
issue, we propose Y2Seq2Seq, a view-based model, to
learn cross-modal representations by joint reconstruction
and prediction of view and word sequences. Specifically,
the network architecture of Y2Seq2Seq bridges
the semantic meaning embedded in the two modalities
by two coupled “Y” like sequence-to-sequence
(Seq2Seq) structures. In addition, our novel hierarchical
constraints further increase the discriminability of
the cross-modal representations by employing more detailed
discriminative information. Experimental results
on cross-modal retrieval and 3D shape captioning show
that Y2Seq2Seq outperforms the state-of-the-art methods.
				</p>
			</div>

			<div class="itemName"> Links:</div>
            <div class="itemContent"> 
				<!-- Platform Prototype [<a href="http://166.111.80.191:8001/tag">BIMTag</a>] -->
				Paper [<a href="../main/pdf/LiuYS_AAAI19_Y2Seq2Seq.pdf">506KB</a>]
				<br>
				PPT [<a href="files/Y2Seq2Seq_AAAI2019.pptx" download="Y2Seq2Seq_AAAI2019.pptx">Y2Seq2Seq_AAAI2019.pptx</a>]
				<!--Code [<a href="SeqViews2SeqLabels.zip" download="SeqViews2SeqLabels.zip">download</a>][<a href="https://github.com/mingyangShang/SeqViews2SeqLabels">github</a>]-->
				<br />
				Dataset [<a href="http://text2shape.stanford.edu/">Primitives and ShapeNet</a>]
			</div>

			<div class="itemName"> Result:</div>
			<div class="itemContent" style="text-align:center">
				<a href="big/Primitives_retrieval.png"><img src="big/Primitives_retrieval.png" width="500" alt="Primitives retrieval"></a>
						<blockquote class="it">
						Figure 2: Primitives cross-modal retrieval result.
						</blockquote>
				<a href="big/Shapenet_retrieval.png"><img src="big/Shapenet_retrieval.png" width="500" alt="Shapenet retrieval"></a>
						<blockquote class="it">
						Figure 3: ShapeNet cross-modal retrieval result.
						</blockquote>
			</div>

			<div class="itemName">Acknowledgements:</div>
			<div class="itemContent">
				Yu-Shen Liu is the corresponding author. This work
was supported by National Key R&D Program of China
(2018YFB0505400), the National Natural Science Foundation
of China (61472202), and Swiss National Science Foundation
grant (169151). We thank all anonymous reviewers
for their constructive comments.
			</div>
			<hr/>
			<div class="itemName">Citation:</div>
			<div class="itemContent">
				If you find this work and source code useful, please cite the following paper:<br /><br />
				&nbsp;&nbsp;&nbsp;&nbsp;
Zhizhong Han, Mingyang Shang, Xiyang Wang, Yu-Shen Liu, Matthias Zwicker. Y^2Seq2Seq: Cross-Modal Representation Learning for 3D Shape and Text by
Joint Reconstruction and Prediction of View and Word Sequences. AAAI, 2019.
				
			</div>
			<hr/>
			<div class="itemName">Contact:</div>
			<div class="itemContent">
				<a name="comm" id="comm"></a>
				Dr. Yu-Shen Liu, Email address: <a href="mailto:liuyushen@tsinghua.edu.cn">liuyushen@tsinghua.edu.cn</a>.<br>
				Zhizhong Han, Email address: <a href="mailto:h312h@mail.nwpu.edu.cn">h312h@mail.nwpu.edu.cn</a>.<br>
				Mingyang Shang, Email address: <a href="mailto:smy16@mails.tsinghua.edu.cn">smy16@mails.tsinghua.edu.cn</a>.
			</div>
		</div>	
	<script src="../common/js/jquery-1.5.min.js" type="text/javascript" ></script>
	<script type="text/javascript" src="../common/js/fancybox/jquery.mousewheel-3.0.4.pack.js"></script>
	<script type="text/javascript" src="../common/js/fancybox/jquery.fancybox-1.3.4.pack.js"></script>	
	<link rel="stylesheet" type="text/css" href="../common/js/fancybox/jquery.fancybox-1.3.4.css" media="screen">
	<script type="text/javascript" >
	<!--
		$(function() {
			$("a:has(img):not(.down)").fancybox({
				'overlayShow'	: false,
				'transitionIn'	: 'elastic',
				'transitionOut'	: 'elastic'
			});
		})
	-->
	</script>
	</body>
</html>
